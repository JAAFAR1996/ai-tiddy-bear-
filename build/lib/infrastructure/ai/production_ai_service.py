"""from datetime import datetime, timedeltafrom typing import Optional, Dict, Any, Listimport asyncioimport hashlibimport jsonimport loggingimport timefrom openai import AsyncOpenAIfrom pydantic import BaseModel, Fieldimport redis.asyncio as redisfrom src.domain.value_objects.child_age import ChildAgefrom src.domain.value_objects.safety_level import SafetyLevelfrom fastapi import Dependsfrom src.infrastructure.config.settings import Settings, get_settingsfrom src.infrastructure.logging import get_standard_loggerfrom src.infrastructure.resilience import retry_external_api, circuit_breaker"""Production-grade AI service with OpenAI GPT-4Enterprise-level implementation with comprehensive error handling, caching, and monitoring."""logger = get_standard_logger(__name__)class AIResponse(BaseModel):    """Structured AI response with safety and quality metrics."""    content: str    safety_score: float = Field(ge=0, le=1)    age_appropriate: bool    sentiment: str    topics: List[str]    processing_time: float    cached: bool = False    moderation_result: Dict[str, Any] = Field(default_factory=dict)class ProductionAIService:    """    Production-grade AI service with comprehensive features:    - Real OpenAI GPT-4 integration    - Multi-layer content filtering    - Age-appropriate responses    - Redis caching for cost optimization    - Comprehensive error handling    - Performance monitoring    - COPPA compliance    """    def __init__(self, settings: Settings = Depends(get_settings)) -> None:        self.settings = settings        api_key = self.settings.ai.OPENAI_API_KEY        if not api_key or not api_key.startswith("sk-"):            raise ValueError("Invalid OpenAI API key format - must start with 'sk-'")        logger.system_startup(f"Initializing OpenAI client with key: sk-***{api_key[-4:]}",                             service="production_ai_service")        import os        os.environ["OPENAI_API_KEY"] = api_key  # Set for OpenAI client        self.client = AsyncOpenAI()  # Client will use env var automatically        del api_key        self.redis = redis.from_url(self.settings.redis.REDIS_URL, decode_responses=True)        self.model = "gpt-4-turbo-preview"        self.max_response_tokens = 200        self.max_story_tokens = 500        self.cache_ttl = 3600  # 1 hour cache        # Age-specific prompts        self.age_prompts = {            ChildAge.TODDLER: "You are a gentle, simple AI friend for very young children (2-4 years). Use very simple words, short sentences, and focus on basic concepts like colors, shapes, and animals.",            ChildAge.PRESCHOOL: "You are a friendly AI companion for preschool children (4-6 years). Use simple vocabulary, ask engaging questions, and encourage learning through play.",            ChildAge.SCHOOL_AGE: "You are an encouraging AI tutor for school-age children (6-12 years). Help with learning, answer questions clearly, and promote curiosity and creativity.",            ChildAge.TWEEN: "You are a supportive AI mentor for tweens (10-13 years). Provide thoughtful guidance, respect their growing independence, and discuss age-appropriate topics."        }    async def generate_response(        self,        message: str,        child_age: ChildAge,        child_name: str,        context: Optional[List[Dict[str, str]]] = None,        parent_guidelines: Optional[str] = None    ) -> AIResponse:        """Generate safe, age-appropriate AI response with comprehensive filtering."""        start_time = time.time()        logger.ai_request_started("Generating AI response",                                user_message_length=len(message),                                child_age=child_age,                                child_name=child_name)        try:            # 1. Input validation and safety check            await self._validate_input(message)            # 2. Check cache first            cache_key = self._generate_cache_key(message, child_age, child_name)            cached_response = await self._get_cached_response(cache_key)            if cached_response:                cached_response.cached = True                return cached_response            # 3. Content moderation check            moderation_result = await self._moderate_content(message)            if not moderation_result["safe"]:                return await self._create_safety_response(moderation_result)            # 4. Generate system prompt based on age            system_prompt = self._create_system_prompt(child_age, child_name, parent_guidelines)            # 5. Prepare conversation context            messages = self._prepare_messages(system_prompt, message, context)            # 6. Call OpenAI API            ai_response = await self._call_openai(messages, child_age)            # 7. Post-process and validate response            processed_response = await self._process_response(                ai_response, moderation_result, start_time, child_age            )            # 8. Cache the response            await self._cache_response(cache_key, processed_response)            duration_ms = (time.time() - start_time) * 1000            logger.ai_request_completed("AI response generated successfully",                                      duration_ms=duration_ms,                                      safety_score=processed_response.safety_score,                                      age_appropriate=processed_response.age_appropriate)            return processed_response        except Exception as e:            logger.ai_request_failed(f"AI service error during response generation",                                   user_message=message, child_age=child_age)            return await self._create_error_response(start_time)    async def _validate_input(self, message: str) -> None:        """Validate input message for safety and compliance."""        if not message or len(message.strip()) == 0:            raise ValueError("Message cannot be empty")        if len(message) > 1000:            raise ValueError("Message too long")        # Check for prohibited content patterns        prohibited_patterns = [            "personal information", "address", "phone", "password",            "credit card", "social security", "unsafe", "dangerous"        ]        message_lower = message.lower()        for pattern in prohibited_patterns:            if pattern in message_lower:                raise ValueError(f"Message contains prohibited content: {pattern}")    async def _moderate_content(self, content: str) -> Dict[str, Any]:        """Use OpenAI moderation API to check content safety."""        try:            response = await self.client.moderations.create(input=content)            result = response.results[0]            return {                "safe": not result.flagged,                "categories": result.categories.model_dump(),                "category_scores": result.category_scores.model_dump()            }        except Exception as e:            logger.warning(f"Moderation API error: {e}")            # Fail safe - assume unsafe if moderation fails            return {"safe": False, "error": str(e)}    def _create_system_prompt(        self,        child_age: ChildAge,        child_name: str,        parent_guidelines: Optional[str] = None    ) -> str:        """Create age-appropriate system prompt."""        base_prompt = self.age_prompts.get(child_age, self.age_prompts[ChildAge.SCHOOL_AGE])        safety_rules = """        CRITICAL SAFETY RULES:        - Never ask for or mention personal information (real name, address, phone, school)        - Keep all content G-rated and educational        - If unsure about appropriateness, choose the safer option        - Encourage positive behaviors and learning        - Redirect inappropriate topics to safe alternatives        - Always maintain a encouraging, supportive tone        """        personalization = f"The child's name is {child_name}. Make responses personal but safe."        guidelines = f"\nParent guidelines: {parent_guidelines}" if parent_guidelines else ""        return f"{base_prompt}\n\n{safety_rules}\n\n{personalization}{guidelines}"    def _prepare_messages(        self,        system_prompt: str,        message: str,        context: Optional[List[Dict[str, str]]] = None    ) -> List[Dict[str, str]]:        """Prepare conversation messages for OpenAI API."""        messages = [{"role": "system", "content": system_prompt}]        # Add recent context (last 3 exchanges)        if context:            recent_context = context[-6:]  # Last 3 exchanges (user + assistant)            messages.extend(recent_context)        messages.append({"role": "user", "content": message})        return messages    @retry_external_api(max_attempts=3, base_delay=1.0)    @circuit_breaker(failure_threshold=5, timeout=300.0)    async def _call_openai(self, messages: List[Dict[str, str]], child_age: ChildAge) -> str:        """Call OpenAI API with appropriate parameters and resilience patterns."""        max_tokens = self.max_response_tokens        if child_age == ChildAge.TODDLER:            max_tokens = 100  # Shorter responses for very young children        try:            response = await self.client.chat.completions.create(                model=self.model,                messages=messages,                max_tokens=max_tokens,                temperature=0.7,                presence_penalty=0.1,                frequency_penalty=0.1,                timeout=30.0            )            return response.choices[0].message.content.strip()        except Exception as e:            logger.error(f"OpenAI API error: {e}")            # Let retry logic handle the exception            raise    async def _process_response(        self,        ai_response: str,        moderation_result: Dict[str, Any],        start_time: float,        child_age: ChildAge    ) -> AIResponse:        """Process and validate AI response."""        processing_time = time.time() - start_time        # Re-moderate the AI response        response_moderation = await self._moderate_content(ai_response)        # Calculate safety score        safety_score = self._calculate_safety_score(response_moderation)        # Determine if age-appropriate        age_appropriate = self._is_age_appropriate(ai_response, child_age)        # Analyze sentiment        sentiment = self._analyze_sentiment(ai_response)        # Extract topics        topics = self._extract_topics(ai_response)        return AIResponse(            content=ai_response,            safety_score=safety_score,            age_appropriate=age_appropriate,            sentiment=sentiment,            topics=topics,            processing_time=processing_time,            moderation_result=response_moderation        )    def _calculate_safety_score(self, moderation_result: Dict[str, Any]) -> float:        """Calculate safety score based on moderation results."""        if not moderation_result.get("safe", False):            return 0.0        if "category_scores" not in moderation_result:            return 0.9  # Default safe score        scores = moderation_result["category_scores"]        max_score = max(scores.values()) if scores else 0.0        # Convert to safety score (inverse of risk score)        return max(0.0, 1.0 - (max_score * 2))    def _is_age_appropriate(self, content: str, child_age: ChildAge) -> bool:        """Check if content is appropriate for the child's age."""        content_lower = content.lower()        # Age-specific inappropriate terms        inappropriate_terms = {            ChildAge.TODDLER: ["complex", "difficult", "scary", "death", "violence"],            ChildAge.PRESCHOOL: ["death", "violence", "scary", "complicated"],            ChildAge.SCHOOL_AGE: ["violence", "inappropriate", "adult"],            ChildAge.TWEEN: ["explicit", "adult", "inappropriate"]        }        terms_to_check = inappropriate_terms.get(child_age, [])        for term in terms_to_check:            if term in content_lower:                return False        return True    def _analyze_sentiment(self, content: str) -> str:        """Simple sentiment analysis."""        positive_words = ["happy", "fun", "great", "wonderful", "amazing", "good", "nice"]        negative_words = ["sad", "bad", "scary", "difficult", "wrong", "no", "stop"]        content_lower = content.lower()        positive_count = sum(1 for word in positive_words if word in content_lower)        negative_count = sum(1 for word in negative_words if word in content_lower)        if positive_count > negative_count:            return "positive"        elif negative_count > positive_count:            return "negative"        else:            return "neutral"    def _extract_topics(self, content: str) -> List[str]:        """Extract main topics from content."""        topics = []        topic_keywords = {            "animals": ["dog", "cat", "animal", "pet", "zoo", "farm"],            "learning": ["learn", "study", "school", "math", "reading", "science"],            "play": ["play", "game", "toy", "fun", "playground"],            "nature": ["tree", "flower", "garden", "outside", "nature", "weather"],            "family": ["family", "mom", "dad", "brother", "sister", "grandma"],            "food": ["food", "eat", "hungry", "dinner", "lunch", "snack"]        }        content_lower = content.lower()        for topic, keywords in topic_keywords.items():            if any(keyword in content_lower for keyword in keywords):                topics.append(topic)        return topics[:3]  # Return top 3 topics    def _generate_cache_key(self, message: str, child_age: ChildAge, child_name: str) -> str:        """Generate cache key for response caching."""        content = f"{message}:{child_age.value}:{child_name}"        return f"ai_response:{hashlib.sha256(content.encode()).hexdigest()[:16]}"    async def _get_cached_response(self, cache_key: str) -> Optional[AIResponse]:        """Get cached response if available."""        try:            cached = await self.redis.get(cache_key)            if cached:                data = json.loads(cached)                return AIResponse(**data)        except Exception as e:            logger.warning(f"Cache retrieval error: {e}")        return None    async def _cache_response(self, cache_key: str, response: AIResponse) -> None:        """Cache the response for future use."""        try:            data = response.model_dump()            await self.redis.setex(cache_key, self.cache_ttl, json.dumps(data))        except Exception as e:            logger.warning(f"Cache storage error: {e}")    async def _create_safety_response(self, moderation_result: Dict[str, Any]) -> AIResponse:        """Create a safe response when content is flagged."""        safe_responses = [            "I'd love to help you with something fun and safe! What would you like to learn about today?",            "Let's talk about something positive! What's your favorite animal or hobby?",            "That's not something I can help with, but I'd be happy to tell you a fun story or answer a question about science!"        ]        import random        content = random.choice(safe_responses)        return AIResponse(            content=content,            safety_score=1.0,            age_appropriate=True,            sentiment="positive",            topics=["safety"],            processing_time=0.001,            moderation_result=moderation_result        )    async def _create_error_response(self, start_time: float) -> AIResponse:        """Create a response when an error occurs."""        processing_time = time.time() - start_time        return AIResponse(            content="I'm having a little trouble right now. Could you try asking me something else?",            safety_score=1.0,            age_appropriate=True,            sentiment="neutral",            topics=["error"],            processing_time=processing_time,            moderation_result={"safe": True}        )    async def generate_story(        self,        theme: str,        child_age: ChildAge,        child_name: str,        length: str = "short"    ) -> AIResponse:        """Generate age-appropriate stories."""        length_tokens = {            "short": 150,            "medium": 300,            "long": 500        }        max_tokens = length_tokens.get(length, 150)        story_prompt = f"Create a {length} story about {theme} for {child_name}. Make it age-appropriate, educational, and fun."        return await self.generate_response(            message=story_prompt,            child_age=child_age,            child_name=child_name        )    async def close(self) -> None:        """Clean up resources."""        try:            await self.redis.close()        except Exception as e:            logger.warning(f"Error closing Redis connection: {e}")