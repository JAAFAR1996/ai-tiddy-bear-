"""Performance Caching Decorators - Prevent N+1 queries and improve response times"""import asyncioimport hashlibimport jsonimport loggingimport timefrom dataclasses import dataclassfrom functools import wrapsfrom typing import Any, Callable, Dict, List, Optional, Unionfrom src.infrastructure.logging_config import get_loggerlogger = get_logger(__name__, component="performance")@dataclassclass CacheConfig:    """Configuration for caching behavior."""    ttl_seconds: int = 300  # 5 minutes default    max_size: int = 1000    key_prefix: str = ""    serialize_complex_types: bool = True    child_safe: bool = True  # Extra privacy for child dataclass InMemoryCache:    """Thread-safe in-memory cache implementation."""    def __init__(self):        self._cache: Dict[str, Dict[str, Any]] = {}        self._access_times: Dict[str, float] = {}        self._lock = asyncio.Lock()    async def get(self, key: str) -> Optional[Any]:        """Get value from cache."""        async with self._lock:            if key not in self._cache:                return None            cache_entry = self._cache[key]            expires_at = cache_entry.get("expires_at", 0)            if time.time() > expires_at:                # Entry expired                del self._cache[key]                if key in self._access_times:                    del self._access_times[key]                return None            self._access_times[key] = time.time()            return cache_entry["value"]    async def set(self, key: str, value: Any, ttl_seconds: int = 300):        """Set value in cache with TTL."""        async with self._lock:            expires_at = time.time() + ttl_seconds            self._cache[key] = {"value": value, "expires_at": expires_at}            self._access_times[key] = time.time()            # Simple LRU eviction if cache is too large            if len(self._cache) > 1000:                await self._evict_oldest()    async def delete(self, key: str):        """Delete key from cache."""        async with self._lock:            if key in self._cache:                del self._cache[key]            if key in self._access_times:                del self._access_times[key]    async def clear(self):        """Clear entire cache."""        async with self._lock:            self._cache.clear()            self._access_times.clear()    async def _evict_oldest(self):        """Evict oldest accessed entries."""        if not self._access_times:            return        # Sort by access time and remove oldest 10%        sorted_keys = sorted(self._access_times.items(), key=lambda x: x[1])        evict_count = max(1, len(sorted_keys) // 10)        for key, _ in sorted_keys[:evict_count]:            if key in self._cache:                del self._cache[key]            if key in self._access_times:                del self._access_times[key]# Global cache instance_cache = InMemoryCache()def _generate_cache_key(    func_name: str, args: tuple, kwargs: dict, prefix: str = "") -> str:    """Generate a stable cache key from function arguments."""    # Create a dictionary with all arguments    arg_dict = {"func": func_name, "args": args, "kwargs": kwargs}    # Serialize to JSON for stable key generation    try:        serialized = json.dumps(arg_dict, sort_keys=True, default=str)    except (TypeError, ValueError):        # Fallback to string representation        serialized = f"{func_name}:{str(args)}:{str(kwargs)}"    # Create hash    key_hash = hashlib.md5(serialized.encode()).hexdigest()    return f"{prefix}:{key_hash}" if prefix else key_hashdef cached(    ttl_seconds: int = 300,    key_prefix: str = "",    max_size: int = 1000,    child_safe: bool = True,):    """    Caching decorator for async functions.    Args:        ttl_seconds: Time to live in seconds        key_prefix: Prefix for cache keys        max_size: Maximum cache size        child_safe: If True, uses shorter TTL and more careful caching for child data    """    def decorator(func: Callable):        @wraps(func)        async def wrapper(*args, **kwargs):            # Adjust TTL for child safety            actual_ttl = min(ttl_seconds, 60) if child_safe else ttl_seconds            # Generate cache key            cache_key = _generate_cache_key(func.__name__, args, kwargs, key_prefix)            # Try to get from cache            cached_result = await _cache.get(cache_key)            if cached_result is not None:                logger.debug(f"Cache hit for {func.__name__}")                return cached_result            # Cache miss - execute function            logger.debug(f"Cache miss for {func.__name__}")            result = await func(*args, **kwargs)            # Cache the result (don't cache None or exceptions)            if result is not None:                await _cache.set(cache_key, result, actual_ttl)            return result        # Add cache management methods        wrapper._cache_clear = lambda: _cache.clear()        wrapper._cache_delete = lambda key: _cache.delete(key)        return wrapper    return decoratordef cache_child_data(ttl_seconds: int = 60):    """    Specialized caching decorator for child-related data.    Uses shorter TTL and child-safe settings.    """    return cached(ttl_seconds=ttl_seconds, key_prefix="child", child_safe=True)def cache_ai_responses(ttl_seconds: int = 300):    """    Caching decorator for AI responses.    Helps prevent repeated API calls for similar requests.    """    return cached(ttl_seconds=ttl_seconds, key_prefix="ai_response", child_safe=True)def cache_safety_checks(ttl_seconds: int = 120):    """    Caching decorator for safety validation results.    """    return cached(ttl_seconds=ttl_seconds, key_prefix="safety", child_safe=True)# Utility functions for cache managementasync def clear_all_caches():    """Clear all caches."""    await _cache.clear()    logger.info("All caches cleared")async def clear_child_cache():    """Clear only child-related cache entries."""    # This is a simplified version - in a real implementation,    # you'd iterate through cache keys and remove child-specific ones    await _cache.clear()    logger.info("Child cache cleared")def cache_stats() -> Dict[str, Any]:    """Get cache statistics."""    return {        "cache_size": len(_cache._cache),        "cache_access_times": len(_cache._access_times),    }